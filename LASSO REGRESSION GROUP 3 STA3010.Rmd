---
title: "LASSO REGRESSION"
author: "MAUREEN MAINA, ID:667806"
date: "2024-09-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readxl)
data("mtcars")
```

**LASSO REGRESSION ON MTCARS DATA**

**To perform lasso regression, we’ll use functions from the glmnet package. This package requires the response variable to be a vector and the set of predictor variables to be of the class data.matrix.**

**Next, we’ll use the glmnet() function to fit the lasso regression model and specify alpha=1. Note that setting alpha equal to 0 is equivalent to using ridge regression and setting alpha to some value between 0 and 1 is equivalent to using an elastic net.**

**To determine what value to use for lambda, we’ll perform k-fold cross-validation and identify the lambda value that produces the lowest test mean squared error (MSE).**

**Note that the function cv.glmnet() automatically performs k-fold cross validation using k = 10 folds.**

```{r}
#LASSO REGRESSION ON MTCARS WHERE HP IS OUR RESPONSE VARIABLE AND MPG,WT,DRAT,QSEC AS PREDICTOR VARIABLES
y<-mtcars$hp
x<-data.matrix(mtcars[,c('mpg','cyl','disp','drat','wt','qsec','gear','carb')])#for lasso X has to be in matrix form

#CHECKING FOR MULTICOLLINEARITY, WHICH WE NOTED THAT THIS DATASET HAS MULTICOLINEARITY AND THEREFORE LASSO REGRESSIONS WILL PERFORM WELL AND WILL BE ABLE TO EXPLAIN THE VARIABILITY OF THE DATA AND PREDICT DATA WELL

library("Hmisc")
pval <- rcorr(as.matrix(mtcars))
print(round(pval$r,2))
library(glmnet)

#perform k-fold cross-validation to find optimal lambda value
cv_model<-cv.glmnet(x,y,alpha=1)

#find optimal lamda value that minimizes test MSE
#set.seed(354)
best_lambda<-cv_model$lambda.min
best_lambda

#produce plot of test MSE by lamda value this code fits the Lasso model across a range of lambda values and performs cross-validation to assess model performance. The plot function shows the mean squared error for each lambda.
plot(cv_model)

#find coefficiencts of best model
best_model<-glmnet(x,y,alpha=1,lambda=best_lambda)
coef(best_model)


```

**No coefficient is shown for the predictor drat,wt & gear because the lasso regression shrunk the coefficient all the way to zero. This means it was completely dropped from the model because it wasn’t influential enough.**

**Note that this is a key difference between ridge regression and lasso regression. Ridge regression shrinks all coefficients towards zero, but lasso regression has the potential to remove predictors from the model by shrinking the coefficients completely to zero.**

**We can also use the final lasso regression model to make predictions on new observations. For example, suppose we have a new car with the following attributes:**

**mpg: 24 cyl: 8 disp: 230 drat: 3.50 wt: 2.5 qsec: 18.5 gear: 5 carb: 3**

**The following code shows how to use the fitted lasso regression model to predict the value for hp of this new observation:**

**Based on the input values, the model predicts this car to have an hp value of 151.0831.**

**The R-squared turns out to be 0.871. That is, the best model was able to explain 87.1% of the variation in the response values of the training data.**

```{r}
#define new observation
new<-matrix(c(24,8,230,3.5,2.5,18.5,5,3),nrow=1,ncol=8)
#use lasso regression to predict response value
predict(best_model,s=best_lambda,newx=new)

#use fitted best model to make predictions
y_predicted <- predict(best_model, s = best_lambda, newx = x)

#find SST and SSE
sst <- sum((y - mean(y))^2)
sse <- sum((y_predicted - y)^2)

#find R-Squared
rsq <- 1 - sse/sst
rsq
```

COMPARISSON OF: LINEAR, RIDGE AND LASSO REGRESSION MODEL COMPARISONS DONE ON STUDENT HEALTH SURVEY DATA

```{r}
# Load necessary libraries
library(readxl)    # For reading Excel files
#install.packages("caret")
library(caret)     # For data preprocessing and MSE calculation
library(glmnet)    # For Ridge and Lasso regression
library(dplyr)     # For data manipulation

# Step 1: Load the dataset
data2 <- read_excel("C:/Users/Hp 830 i7/Desktop/DATA SCIENCE/Student health survey data.xlsx")
#CLEANED THE DATA FIRST
data2$weight[data2$weight == 9999] <- NA
data2 <- na.omit(data2);#View(data)
#View(data2)
# View the first few rows to understand the structure
#head(data)

# Step 2: Preprocess the data
# Remove 'sex' column and handle missing data if necessary
data2 <- data2 %>% select(-sex)  # Remove 'sex' column

# Check for missing values

data2 <- na.omit(data2)  # Remove rows with missing data
y_d<-data2$incmnth
X_d <- data.matrix(data2[,c('age','homecost','mobilecost','transport','food','entertain','veg','fruit','height','weight')])
print("Original Data")

#CHECKING FOR MULTICOLLINEARITY, WHICH WE NOTED THAT THIS DATASET HAS NO MULTICOLINEARITY AND THEREFORE OUR RIDGE AND LASSO REGRESSIONS WONT PERFORM WELL OR WON'T BE ABLE TO EXPLAIN THE VARIABILITY OF THE DATA, HOWEVER FOR ILLUSTRATION PER

library("Hmisc")
p_values <- rcorr(as.matrix(data2))
print(p_values)

# LINEAR REGRESSION
lm_model <- lm(incmnth ~age+homecost+mobilecost+transport+food+entertain+veg+fruit+height+weight, data = data2)
summary(lm_model)

# Linear Regression Performance
lm_predictions <- predict(lm_model, data2)
lm_mse <- mean((lm_predictions - y_d)^2)
lm_rmse <- sqrt(lm_mse)
lm_rsq <- summary(lm_model)$r.squared

# Display Linear Regression Performance
cat("Linear Regression Model Performance:\n")
cat("MSE:", lm_mse, "\n")
cat("RMSE:", lm_rmse, "\n")
cat("R-squared:", lm_rsq, "\n\n")

# RIDGE REGRESSION
ridge_model <- cv.glmnet(X_d, y_d, alpha = 0)  # alpha=0 for ridge
best_lambda_ridge <- ridge_model$lambda.min
ridge_model2 <- glmnet(X_d, y_d, alpha = 0,lambda=best_lambda_ridge) 
coef(ridge_model2)

# Ridge Regression Performance
ridge_predictions <- predict(ridge_model2, s = best_lambda_ridge, newx = X_d)
ridge_mse <- mean((ridge_predictions - y_d)^2)
ridge_rmse <- sqrt(ridge_mse)
ridge_rsq <- 1 - (sum((ridge_predictions - y_d)^2) / sum((y_d - mean(y_d))^2))

# Display Ridge Regression Performance
cat("Ridge Regression Model Performance:\n")
cat("MSE:", ridge_mse, "\n")
cat("RMSE:", ridge_rmse, "\n")
cat("R-squared:", ridge_rsq, "\n\n")

#LASSO REGRESSION

lasso_model <- cv.glmnet(X_d, y_d, alpha = 1)  # alpha=1 for lasso
best_lambda_lasso <- lasso_model$lambda.min
lasso_model2 <- glmnet(X_d, y_d, alpha = 1,lambda=best_lambda_lasso)
coef(lasso_model2)

# Lasso Regression Performance
lasso_predictions <- predict(lasso_model2, s = best_lambda_lasso, newx = X_d)
lasso_mse <- mean((lasso_predictions - y_d)^2)
lasso_rmse <- sqrt(lasso_mse)
lasso_rsq <- 1 - (sum((lasso_predictions - y_d)^2) / sum((y_d - mean(y_d))^2))

# Display Lasso Regression Performance
cat("Lasso Regression Model Performance:\n")
cat("MSE:", lasso_mse, "\n")
cat("RMSE:", lasso_rmse, "\n")
cat("R-squared:", lasso_rsq, "\n\n")

# Compare performance
performance <- data.frame(
  Model = c("Linear Regression", "Ridge Regression", "Lasso Regression"),
  MSE = c(lm_mse, ridge_mse, lasso_mse),
  RMSE = c(lm_rmse, ridge_rmse, lasso_rmse),
  R_Squared = c(lm_rsq, ridge_rsq, lasso_rsq)
)

print(performance)

#LETS TRY PREDICT THE INCOME PER MONTH USING RIDGE AND LASSO REGRESSION FOR NEW DATA ADDED TO THE DATASET

# New data for prediction
new_data <- data.frame(
  age = c(25, 30, 22),
  homecost = c(1200, 1500, 1000),
  mobilecost = c(60, 50, 70),
  transport = c(100, 150, 80),
  food = c(300, 400, 250),
  entertain = c(200, 250, 150),
  veg = c(5, 6, 7),
  fruit = c(4, 3, 5),
  height = c(175, 180, 165),
  weight = c(70, 80, 65)
)
new_data
# Step 1: Predict with Ridge Regression
ridge_predictions <- predict(ridge_model, s = best_lambda_ridge, newx = as.matrix(new_data))
cat("Ridge Regression Predictions for incmnth:")
print(ridge_predictions)

# Step 2: Predict with Lasso Regression
lasso_predictions <- predict(lasso_model, s = best_lambda_lasso, newx = as.matrix(new_data))
cat("Lasso Regression Predictions for incmnth:")
print(lasso_predictions)

```

Our linear regression model is able to explain about 18% of the variation in the data. While it’s statistically significant, it means the model doesn't capture the majority of the variability in the response variable. On average, our predictions are off by about 1264 units. This suggests that there’s still a lot of unexplained variation, and we may need to explore other variables or models to improve the predictions.

NB: OUR DATASET WASN'T VERY GOOD, IT HAD ALOT OF ZERO VALUES WHICH WERE GREATLY AFFECTING OF THE LINEAR REGRESSION HOWEVER DUE TO THE NON MUTLICOLLIENARITY LINEAR REGRESSION IS THE BEST MODEL
